    # Step 1: Load the Existing Keywords
import dataiku
import pandas as pd

# Load 'ade_keyword' dataset
ade_keyword_dataset = dataiku.Dataset('ade_keyword')
ade_keywords_df = ade_keyword_dataset.get_dataframe()

# Filter for 'Adverse Event' event_type
ade_keywords_df = ade_keywords_df[ade_keywords_df['event_type'] == 'Adverse Event']

# Extract the list of existing keywords
existing_keywords = ade_keywords_df['keyword_name'].unique().tolist()
print(f"Number of existing keywords: {len(existing_keywords)}")

# Step 2: Load and Preprocess the Call Transcriptions
# Load 'Processed_Master' dataset
processed_master_dataset = dataiku.Dataset("Processed_Master")
df = processed_master_dataset.get_dataframe()

# Combine all text into a list for training
texts = df['Processed_text_Nostpwd'].dropna().tolist()

# Preprocess the text
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import re

# Download NLTK data if not already downloaded
nltk.download('punkt')
nltk.download('stopwords')

stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    # Lowercase
    text = text.lower()
    # Remove special characters and numbers
    text = re.sub(r'[^a-z\s]', '', text)
    # Tokenize
    tokens = word_tokenize(text)
    # Remove stopwords
    tokens = [word for word in tokens if word not in stop_words]
    return tokens

# Apply preprocessing
processed_texts = [preprocess_text(text) for text in texts]

# Step 3: Train Word2Vec Model
from gensim.models import Word2Vec

# Train the model
model = Word2Vec(sentences=processed_texts, vector_size=100, window=5, min_count=5, workers=4)

# Step 4: Find Similar Words
similar_words = set()

for keyword in existing_keywords:
    keyword = keyword.lower()
    if keyword in model.wv:
        # Get top 10 similar words
        similar = model.wv.most_similar(keyword, topn=10)
        similar_words.update([word for word, score in similar])
    else:
        print(f"Keyword '{keyword}' not found in vocabulary.")

# Combine existing keywords with similar words
expanded_keywords = set(existing_keywords)
expanded_keywords.update(similar_words)

print(f"Number of expanded keywords: {len(expanded_keywords)}")

# Step 5: Save the Expanded Keyword List
# Convert to DataFrame
expanded_keywords_df = pd.DataFrame({'keyword_name': list(expanded_keywords)})

# Save to Dataiku dataset
expanded_keywords_dataset = dataiku.Dataset("expanded_ade_keywords")
expanded_keywords_dataset.write_with_schema(expanded_keywords_df)






# Step 1: Load and Preprocess the Data
import dataiku
import pandas as pd
import nltk
import re
from sklearn.feature_extraction.text import CountVectorizer

# Load 'Processed_Master' dataset
processed_master_dataset = dataiku.Dataset("Processed_Master")
df = processed_master_dataset.get_dataframe()

# Define the categories based on the flags
categories = {
    'Technical complaints': df[df['TECHNICAL COMPLAINT'] == 1],
    'AE': df[(df['ADVERSE EVENT'] == 1) | (df['SERIOUS ADVERSE EVENT'] == 1)],
    'Safety Only': df[df['SAFETY ONLY'] == 1]
}

# Ensure NLTK data is downloaded
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Initialize lemmatizer and stopwords
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
stop_words = set(nltk.corpus.stopwords.words('english'))

# Add custom stopwords (if any)
custom_stopwords = set(['patient', 'doctor', 'medicine', 'drug', 'use', 'take', 'get', 'feel', 'would', 'could'])
stop_words.update(custom_stopwords)

def preprocess_text(text):
    # Lowercase
    text = text.lower()
    # Remove special characters and numbers
    text = re.sub(r'[^a-z\s]', '', text)
    # Tokenize
    tokens = nltk.word_tokenize(text)
    # Remove stopwords
    tokens = [word for word in tokens if word not in stop_words]
    # Lemmatize
    tokens = [lemmatizer.lemmatize(word) for word in tokens]
    return ' '.join(tokens)

# Step 2: Process Texts and Extract N-Grams
for category_name, category_df in categories.items():
    print(f"Processing category: {category_name}")
    texts = category_df['Processed_text_Nostpwd'].dropna().tolist()
    # Preprocess texts
    processed_texts = [preprocess_text(text) for text in texts]
    
    # Initialize CountVectorizer for unigrams
    vectorizer_uni = CountVectorizer(ngram_range=(1,1), stop_words='english')
    unigram_counts = vectorizer_uni.fit_transform(processed_texts)
    unigram_features = vectorizer_uni.get_feature_names_out()
    unigram_totals = unigram_counts.sum(axis=0).A1
    unigram_df = pd.DataFrame({'ngram': unigram_features, 'count': unigram_totals})
    unigram_df = unigram_df.sort_values(by='count', ascending=False)
    
    # Initialize CountVectorizer for bigrams
    vectorizer_bi = CountVectorizer(ngram_range=(2,2), stop_words='english')
    bigram_counts = vectorizer_bi.fit_transform(processed_texts)
    bigram_features = vectorizer_bi.get_feature_names_out()
    bigram_totals = bigram_counts.sum(axis=0).A1
    bigram_df = pd.DataFrame({'ngram': bigram_features, 'count': bigram_totals})
    bigram_df = bigram_df.sort_values(by='count', ascending=False)
    
    # Initialize CountVectorizer for trigrams
    vectorizer_tri = CountVectorizer(ngram_range=(3,3), stop_words='english')
    trigram_counts = vectorizer_tri.fit_transform(processed_texts)
    trigram_features = vectorizer_tri.get_feature_names_out()
    trigram_totals = trigram_counts.sum(axis=0).A1
    trigram_df = pd.DataFrame({'ngram': trigram_features, 'count': trigram_totals})
    trigram_df = trigram_df.sort_values(by='count', ascending=False)
    
    # Combine n-grams
    ngram_df = pd.concat([unigram_df, bigram_df, trigram_df], ignore_index=True)
    ngram_df = ngram_df.sort_values(by='count', ascending=False)
    
    # Remove any n-grams containing stopwords or non-informative words
    def is_informative(ngram):
        words = ngram.split()
        return all(word not in stop_words for word in words)
    
    ngram_df = ngram_df[ngram_df['ngram'].apply(is_informative)]
    
    # Select top N n-grams (e.g., top 1000)
    top_n = 1000
    top_ngrams = ngram_df.head(top_n)
    
    # Save to Dataiku dataset
    dataset_name = f"keywords_{category_name.replace(' ', '_')}"
    dataset = dataiku.Dataset(dataset_name)
    dataset.write_with_schema(top_ngrams[['ngram', 'count']])
    
    print(f"Saved top n-grams for category '{category_name}' to dataset '{dataset_name}'")









# Step 1: Preprocess the Data
import dataiku
import pandas as pd
import nltk
import re
from sklearn.feature_extraction.text import CountVectorizer

# If NLTK data is not downloaded, uncomment the following lines:
# nltk.download('punkt')
# nltk.download('stopwords')
# nltk.download('wordnet')

from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# Initialize lemmatizer and stopwords
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

# Add custom stopwords if needed
custom_stopwords = set(['patient', 'doctor', 'medicine', 'drug', 'use', 'take', 'get', 'feel', 'would', 'could'])
stop_words.update(custom_stopwords)

def preprocess_text(text):
    # Convert to lowercase
    text = text.lower()
    # Remove special characters and numbers
    text = re.sub(r'[^a-z\s]', '', text)
    # Tokenize
    tokens = nltk.word_tokenize(text)
    # Remove stopwords
    tokens = [word for word in tokens if word not in stop_words]
    # Lemmatize
    tokens = [lemmatizer.lemmatize(word) for word in tokens]
    # Join tokens back into a string
    return ' '.join(tokens)

# Apply preprocessing to the text column
ae_df['processed_text'] = ae_df['col_1'].apply(preprocess_text)

# Step 2: Extract Unigrams Using CountVectorizer
vectorizer_uni = CountVectorizer(ngram_range=(1,1))
X_uni_counts = vectorizer_uni.fit_transform(ae_df['processed_text'])

# Get the unigram features and their counts
unigram_features = vectorizer_uni.get_feature_names_out()
unigram_counts = X_uni_counts.sum(axis=0).A1  # Sum over all documents to get total counts

# Create a dataframe with unigrams and their counts
unigram_df = pd.DataFrame({'unigram': unigram_features, 'count': unigram_counts})

# Sort the unigrams by frequency
unigram_df = unigram_df.sort_values(by='count', ascending=False)

# Select Top 1500 Unigrams
top_1500_unigrams = unigram_df.head(1500)

# Step 3: Extract Bigrams Using CountVectorizer
vectorizer_bi = CountVectorizer(ngram_range=(2,2))
X_bi_counts = vectorizer_bi.fit_transform(ae_df['processed_text'])

# Get the bigram features and their counts
bigram_features = vectorizer_bi.get_feature_names_out()
bigram_counts = X_bi_counts.sum(axis=0).A1  # Sum over all documents to get total counts

# Create a dataframe with bigrams and their counts
bigram_df = pd.DataFrame({'bigram': bigram_features, 'count': bigram_counts})

# Sort the bigrams by frequency
bigram_df = bigram_df.sort_values(by='count', ascending=False)

# Select Top 1500 Bigrams
top_1500_bigrams = bigram_df.head(1500)

# Step 4: Save the Results to Dataiku Datasets
# Create Dataiku datasets (replace dataset names if needed)
unigrams_dataset = dataiku.Dataset('ae_top_unigrams')
bigrams_dataset = dataiku.Dataset('ae_top_bigrams')

# Write the top unigrams to the dataset
unigrams_dataset.write_with_schema(top_1500_unigrams)

# Write the top bigrams to the dataset
bigrams_dataset.write_with_schema(top_1500_bigrams)

print("Top 1500 unigrams have been saved to the 'ae_top_unigrams' dataset.")
print("Top 1500 bigrams have been saved to the 'ae_top_bigrams' dataset.")

