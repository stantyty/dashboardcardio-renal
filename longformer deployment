    # Step 1: Load the Existing Keywords
import dataiku
import pandas as pd

# Load 'ade_keyword' dataset
ade_keyword_dataset = dataiku.Dataset('ade_keyword')
ade_keywords_df = ade_keyword_dataset.get_dataframe()

# Filter for 'Adverse Event' event_type
ade_keywords_df = ade_keywords_df[ade_keywords_df['event_type'] == 'Adverse Event']

# Extract the list of existing keywords
existing_keywords = ade_keywords_df['keyword_name'].unique().tolist()
print(f"Number of existing keywords: {len(existing_keywords)}")

# Step 2: Load and Preprocess the Call Transcriptions
# Load 'Processed_Master' dataset
processed_master_dataset = dataiku.Dataset("Processed_Master")
df = processed_master_dataset.get_dataframe()

# Combine all text into a list for training
texts = df['Processed_text_Nostpwd'].dropna().tolist()

# Preprocess the text
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import re

# Download NLTK data if not already downloaded
nltk.download('punkt')
nltk.download('stopwords')

stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    # Lowercase
    text = text.lower()
    # Remove special characters and numbers
    text = re.sub(r'[^a-z\s]', '', text)
    # Tokenize
    tokens = word_tokenize(text)
    # Remove stopwords
    tokens = [word for word in tokens if word not in stop_words]
    return tokens

# Apply preprocessing
processed_texts = [preprocess_text(text) for text in texts]

# Step 3: Train Word2Vec Model
from gensim.models import Word2Vec

# Train the model
model = Word2Vec(sentences=processed_texts, vector_size=100, window=5, min_count=5, workers=4)

# Step 4: Find Similar Words
similar_words = set()

for keyword in existing_keywords:
    keyword = keyword.lower()
    if keyword in model.wv:
        # Get top 10 similar words
        similar = model.wv.most_similar(keyword, topn=10)
        similar_words.update([word for word, score in similar])
    else:
        print(f"Keyword '{keyword}' not found in vocabulary.")

# Combine existing keywords with similar words
expanded_keywords = set(existing_keywords)
expanded_keywords.update(similar_words)

print(f"Number of expanded keywords: {len(expanded_keywords)}")

# Step 5: Save the Expanded Keyword List
# Convert to DataFrame
expanded_keywords_df = pd.DataFrame({'keyword_name': list(expanded_keywords)})

# Save to Dataiku dataset
expanded_keywords_dataset = dataiku.Dataset("expanded_ade_keywords")
expanded_keywords_dataset.write_with_schema(expanded_keywords_df)






# Step 1: Load and Preprocess the Data
import dataiku
import pandas as pd
import nltk
import re
from sklearn.feature_extraction.text import CountVectorizer

# Load 'Processed_Master' dataset
processed_master_dataset = dataiku.Dataset("Processed_Master")
df = processed_master_dataset.get_dataframe()

# Define the categories based on the flags
categories = {
    'Technical complaints': df[df['TECHNICAL COMPLAINT'] == 1],
    'AE': df[(df['ADVERSE EVENT'] == 1) | (df['SERIOUS ADVERSE EVENT'] == 1)],
    'Safety Only': df[df['SAFETY ONLY'] == 1]
}

# Ensure NLTK data is downloaded
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Initialize lemmatizer and stopwords
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
stop_words = set(nltk.corpus.stopwords.words('english'))

# Add custom stopwords (if any)
custom_stopwords = set(['patient', 'doctor', 'medicine', 'drug', 'use', 'take', 'get', 'feel', 'would', 'could'])
stop_words.update(custom_stopwords)

def preprocess_text(text):
    # Lowercase
    text = text.lower()
    # Remove special characters and numbers
    text = re.sub(r'[^a-z\s]', '', text)
    # Tokenize
    tokens = nltk.word_tokenize(text)
    # Remove stopwords
    tokens = [word for word in tokens if word not in stop_words]
    # Lemmatize
    tokens = [lemmatizer.lemmatize(word) for word in tokens]
    return ' '.join(tokens)

# Step 2: Process Texts and Extract N-Grams
for category_name, category_df in categories.items():
    print(f"Processing category: {category_name}")
    texts = category_df['Processed_text_Nostpwd'].dropna().tolist()
    # Preprocess texts
    processed_texts = [preprocess_text(text) for text in texts]
    
    # Initialize CountVectorizer for unigrams
    vectorizer_uni = CountVectorizer(ngram_range=(1,1), stop_words='english')
    unigram_counts = vectorizer_uni.fit_transform(processed_texts)
    unigram_features = vectorizer_uni.get_feature_names_out()
    unigram_totals = unigram_counts.sum(axis=0).A1
    unigram_df = pd.DataFrame({'ngram': unigram_features, 'count': unigram_totals})
    unigram_df = unigram_df.sort_values(by='count', ascending=False)
    
    # Initialize CountVectorizer for bigrams
    vectorizer_bi = CountVectorizer(ngram_range=(2,2), stop_words='english')
    bigram_counts = vectorizer_bi.fit_transform(processed_texts)
    bigram_features = vectorizer_bi.get_feature_names_out()
    bigram_totals = bigram_counts.sum(axis=0).A1
    bigram_df = pd.DataFrame({'ngram': bigram_features, 'count': bigram_totals})
    bigram_df = bigram_df.sort_values(by='count', ascending=False)
    
    # Initialize CountVectorizer for trigrams
    vectorizer_tri = CountVectorizer(ngram_range=(3,3), stop_words='english')
    trigram_counts = vectorizer_tri.fit_transform(processed_texts)
    trigram_features = vectorizer_tri.get_feature_names_out()
    trigram_totals = trigram_counts.sum(axis=0).A1
    trigram_df = pd.DataFrame({'ngram': trigram_features, 'count': trigram_totals})
    trigram_df = trigram_df.sort_values(by='count', ascending=False)
    
    # Combine n-grams
    ngram_df = pd.concat([unigram_df, bigram_df, trigram_df], ignore_index=True)
    ngram_df = ngram_df.sort_values(by='count', ascending=False)
    
    # Remove any n-grams containing stopwords or non-informative words
    def is_informative(ngram):
        words = ngram.split()
        return all(word not in stop_words for word in words)
    
    ngram_df = ngram_df[ngram_df['ngram'].apply(is_informative)]
    
    # Select top N n-grams (e.g., top 1000)
    top_n = 1000
    top_ngrams = ngram_df.head(top_n)
    
    # Save to Dataiku dataset
    dataset_name = f"keywords_{category_name.replace(' ', '_')}"
    dataset = dataiku.Dataset(dataset_name)
    dataset.write_with_schema(top_ngrams[['ngram', 'count']])
    
    print(f"Saved top n-grams for category '{category_name}' to dataset '{dataset_name}'")










Category 1: Technical Complaints
Device Failures and Malfunctions
Malfunction
Device failure
Not working
Stopped working
Broken device
Faulty device
Defective insulin pen
Pen malfunction
Device error
Error message
Glitch
Frozen screen
Unresponsive
Intermittent function
Power failure
Battery issue
Calibration problem
Needle jam
Stuck plunger
Dose not dispensing
Inaccurate dosing
Leakage
Leaking insulin
Cracked casing
Button not working
Display issue
Screen flickering
Mechanical failure
Software bug
Overheating
Illegitimacy/Potential Counterfeit
Counterfeit product
Fake insulin pen
Illegitimate device
Unauthorized seller
Suspicious packaging
Tampered seal
Unusual markings
Not authentic
Expired product
Recalled device
Black market
Packaging Errors and Defects
Packaging error
Damaged packaging
Missing components
Incorrect labeling
Mislabeled
Leaking package
Defective packaging
Broken seal
Improper sealing
Contaminated packaging
Opened package
Inadequate instructions
No user manual
Incomplete kit
Wrong product inside
Category 2: Adverse Events
Side Effects and Adverse Reactions
Side effect
Adverse reaction
Allergic reaction
Anaphylaxis
Rash
Hives
Swelling
Itching
Redness
Difficulty breathing
Shortness of breath
Chest pain
Dizziness
Lightheadedness
Headache
Nausea
Vomiting
Diarrhea
Stomach pain
Abdominal cramps
Fatigue
Weakness
Muscle pain
Joint pain
Fever
Chills
Sweating
Blurred vision
Tremors
Palpitations
Anxiety
Outcomes Due to Medication or Device Use
Hypoglycemia
Low blood sugar
Hyperglycemia
High blood sugar
Blood sugar spikes
Blood sugar drops
Seizure
Loss of consciousness
Fainting
Coma
Weight gain
Weight loss
Injection site reaction
Infection
Abscess
Scarring
Lipodystrophy
Lipoatrophy
Skin irritation
Bruising
Suspected Transmission of Infectious Agents
Infection transmission
Contaminated insulin
Bloodborne pathogen
Hepatitis
HIV exposure
Contaminated device
Sterility issue
Sepsis
Bacterial contamination
Viral infection
Category 3: Safety Only
Pregnancy and Exposure
Pregnancy
Pregnant
Expecting
Fetus exposure
Prenatal exposure
Breastfeeding
Nursing mother
Congenital anomaly
Birth defect
Miscarriage
Stillbirth
Preterm labor
Low birth weight
Prenatal care
Gestational diabetes
Overdose, Abuse, and Misuse
Overdose
Accidental overdose
Intentional overdose
Double dose
Extra dose
Missed dose
Forgot dose
Wrong dose
Dose error
Medication error
Drug abuse
Substance abuse
Misuse
Recreational use
Non-prescribed use
Addiction
Dependency
Self-harm
Suicide attempt
Occupational Exposure and Medication Errors
Occupational exposure
Needlestick injury
Accidental exposure
Healthcare worker exposure
Pharmacist error
Dispensing error
Administration error
Wrong medication
Incorrect route
Wrong patient
Wrong time
Improper storage
Expired medication
Labeling error
Instruction error
Dosage miscalculation
Other Safety Concerns
Drug interaction
Contraindication
Off-label use
Unauthorized use
Noncompliance
Non-adherence
Sharing medication
Lost medication
Stolen medication
Illicit use
Unapproved therapy
Experimental use
Clinical trial participation
